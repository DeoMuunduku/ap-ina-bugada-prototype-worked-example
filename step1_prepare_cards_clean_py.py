# -*- coding: utf-8 -*-
"""step1_prepare_cards_clean.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15ZSD-IHirkzs8h6IwF07NzqWNaGz0Jx9
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Étape 1 — Préparation cartes BUGADA/JIRA (PROPRE, anti-leakage)
- Lecture robuste (JSON / JSON dict / JSONL)
- Normalisation cartes (Bugzilla, Jira, fallback)
- Extraction desc_blob (description + 1er commentaire si dispo)
- Détection leakage (status/résolution post-hoc) + raisons  [QC seulement]
- QC: missingness.csv, duplicates.csv
- Figures "papier" + copies -> paper_assets/
- dataset_stats.json + DATACARD.md enrichi (provenance/licence)
- feature_allowlist.txt (liste blanche anti-leakage)
"""

import os, json, sys, csv, hashlib, shutil
from collections import Counter, defaultdict
from datetime import datetime, timezone

# ============ RÉGLAGES (⚠️ adapte ces chemins) ============
INPUT_PATH = "/content/drive/MyDrive/bugdata/bugs.json"
OUT_DIR    = "/content/drive/MyDrive/bugada_cards_clean"
LIMIT      = None  # None = pas de limite
# ===========================================================

# Métadonnées (modifie si tu n’es pas sur BMO)
DATA_SOURCE_NAME = "Bugzilla@Mozilla (BMO)"
DATA_SOURCE_URL  = "https://bugzilla.mozilla.org"
DATA_ACCESS_DATE = "2025-11-05"
DATA_LICENSE     = "Mozilla Websites & Communications Terms of Use (voir mentions sur le site)"
DATA_LICENSE_URL = "https://www.mozilla.org/en-US/about/legal/terms/mozilla/"

# ---------------- Utils ----------------
def now_iso():
    return datetime.now(timezone.utc).replace(microsecond=0).isoformat()

def ensure_dir(p):
    os.makedirs(p, exist_ok=True)

def safe_str(x, default=""):
    if x is None: return default
    try:
        return str(x)
    except Exception:
        return default

def sha256_bytes(b: bytes) -> str:
    return "sha256:" + hashlib.sha256(b).hexdigest()

def sha256_file(path: str) -> str:
    with open(path, "rb") as f:
        return sha256_bytes(f.read())

def sha256_obj(obj) -> str:
    s = json.dumps(obj, ensure_ascii=False, sort_keys=True, separators=(",", ":"))
    return sha256_bytes(s.encode("utf-8"))

# ------------- Lecture robuste -------------
def read_json_any(path):
    with open(path, "r", encoding="utf-8") as f:
        txt = f.read()

    # JSON (liste/dict)
    try:
        data = json.loads(txt)
        if isinstance(data, dict):
            for key in ("bugs","issues","data","items"):
                if key in data and isinstance(data[key], list):
                    print(f"[LOAD] JSON dict avec liste '{key}' -> {len(data[key])} éléments")
                    return data[key]
            print("[LOAD] JSON dict sans clé-liste standard -> 1 élément (dict)")
            return [data]
        if isinstance(data, list):
            print(f"[LOAD] JSON liste -> {len(data)} éléments")
            return data
    except Exception:
        print("[LOAD] Pas un gros JSON, on tente JSONL…")

    # JSONL
    items = []
    for i, line in enumerate(txt.splitlines(), 1):
        line = line.strip()
        if not line: continue
        try:
            obj = json.loads(line)
            if isinstance(obj, dict) and "bug" in obj and isinstance(obj["bug"], dict):
                obj = obj["bug"]
            items.append(obj)
        except Exception:
            continue
    print(f"[LOAD] JSONL -> {len(items)} lignes valides")
    return items

# -------- Détection systèmes --------
def is_jira(bug):
    if "key" in bug: return True
    if "fields" in bug and isinstance(bug["fields"], dict): return True
    if "project" in bug and ("components" in bug or "Component" in bug): return True
    return False

def is_bugzilla(bug):
    return ("id" in bug or "bug_id" in bug) and ("summary" in bug or "short_desc" in bug or "title" in bug)

# -------- Normalisation / util --------
def norm_severity_generic(s):
    s = (s or "").strip().lower()
    repl = {
        "critical":"high","blocker":"high","major":"high",
        "minor":"low","trivial":"low","normal":"medium",
        "--":"unknown","": "unknown","none":"unknown","n/a":"unknown"
    }
    return repl.get(s, s or "unknown")

def norm_severity_bugzilla(s):
    s = (s or "").strip().lower()
    m = {"s1":"high","s2":"high","s3":"medium","s4":"low","s5":"low"}
    return m.get(s, norm_severity_generic(s))

def compute_desc_len(summary, long_text, comments_len_chars=0):
    return len(safe_str(summary)) + len(safe_str(long_text)) + int(comments_len_chars)

# Hints (déterministes; UNIQUEMENT texte)
REGRESSION_HINTS = ["regression","after update","after upgrade","after release","after deploy","since version","introduced in","since build","since release"]
INFRA_HINTS      = ["timeout","latency","slow","slowdown","performance","5xx","server error","network","intermittent","connection reset","502","503","bad gateway"]
INSUFF_HINTS     = ["cannot reproduce","need more info","needinfo","missing steps","incomplete","unconfirmed","no steps"]
BUGZILLA_NEG     = {"worksforme","works as intended","notabug","invalid","works for me","duplicate","moved"}
BUGZILLA_BSE     = {"wontfix","by design","policy decision"}

def extract_keywords_from_text(*texts):
    """Calcule des mots-clés uniquement depuis le TEXTE (pas de status/résolution)."""
    t = " ".join([safe_str(x).lower() for x in texts if x])
    kw = set()
    if any(h in t for h in REGRESSION_HINTS): kw.add("regression")
    for h in INFRA_HINTS:
        if h in t: kw.add(h.split()[0])  # "latency","timeout","slow","5xx","network","intermittent","502","503","bad"
    for h in INSUFF_HINTS:
        if h in t: kw.add("needinfo")
    for r in BUGZILLA_NEG:
        if r in t: kw.add("falsepositive")
    for r in BUGZILLA_BSE:
        if r in t: kw.add("wontfix")
    return sorted(kw)

def has_security_indicator(*texts) -> bool:
    t = " ".join([safe_str(x).lower() for x in texts if x])
    SEC = ("security","vulnerability","xss","csrf","sql injection")
    return any(s in t for s in SEC)

# -------- Extracteurs desc_blob --------
def _first_nonempty(*candidates):
    for c in candidates:
        c = safe_str(c).strip()
        if c:
            return c
    return ""

def _extract_desc_from_bugzilla(bug):
    long_text_fields = [
        bug.get("description"), bug.get("desc"), bug.get("longdesc"),
        bug.get("long_desc"), bug.get("raw_text"), bug.get("text"), bug.get("details"),
    ]
    base = _first_nonempty(*long_text_fields)

    first_comment = ""
    comments_len_chars = 0
    comments = bug.get("comments")
    if isinstance(comments, list) and comments:
        for c in comments:
            ctext = safe_str(c.get("text") or c.get("raw_text") or c.get("body"))
            comments_len_chars += len(ctext)
        first_comment = safe_str(comments[0].get("text") or comments[0].get("raw_text") or comments[0].get("body"))
    elif isinstance(comments, dict):
        arr = comments.get("comments") or comments.get("data") or []
        if isinstance(arr, list) and arr:
            for c in arr:
                ctext = safe_str(c.get("text") or c.get("raw_text") or c.get("body"))
                comments_len_chars += len(ctext)
            first_comment = safe_str(arr[0].get("text") or arr[0].get("raw_text") or arr[0].get("body"))

    desc_blob = base if base else first_comment
    return desc_blob, comments_len_chars

def _extract_desc_from_jira(bug):
    fields = bug.get("fields") if isinstance(bug.get("fields"), dict) else {}
    base = safe_str(fields.get("description") or bug.get("description") or "")
    comments_len_chars = 0
    if "comment" in fields and isinstance(fields["comment"], dict):
        arr = fields["comment"].get("comments") or []
        if isinstance(arr, list):
            for c in arr:
                comments_len_chars += len(safe_str(c.get("body")))
            if not base and arr:
                base = safe_str(arr[0].get("body"))
    return base, comments_len_chars

# -------- Cartes --------
CLOSED_STATUSES = {"resolved","closed","verified","done","fixed"}

def _leakage_from_status_resolution(status, resolution):
    """QC : fuite potentielle si on injecte ça en features (on NE le fera pas)."""
    reasons = []
    st = safe_str(status).strip().lower()
    res = safe_str(resolution).strip().upper()
    if st in CLOSED_STATUSES:
        reasons.append(f"status={status}")
    if res:
        reasons.append(f"resolution={resolution}")
    return (len(reasons) > 0), reasons

def jira_to_card(bug):
    fields = bug.get("fields") if isinstance(bug.get("fields"), dict) else {}

    key = safe_str(bug.get("key") or bug.get("id") or bug.get("ticket_id") or "")
    # project
    if "project" in fields and isinstance(fields["project"], dict) and fields["project"].get("key"):
        project = fields["project"]["key"]
    else:
        project = safe_str(bug.get("project") or "BUGS")

    # component
    component = None
    if "components" in fields and isinstance(fields["components"], list) and fields["components"]:
        c0 = fields["components"][0]
        component = c0.get("name") if isinstance(c0, dict) else str(c0)
    if not component:
        component = bug.get("component") or bug.get("Component") or "General"
        if isinstance(component, list) and component:
            component = component[0]
    component = safe_str(component)

    # created
    created = fields.get("created") or bug.get("created") or bug.get("created_at") or now_iso()
    created = safe_str(created)

    # status / severity / resolution
    status = ""
    if "status" in fields and isinstance(fields["status"], dict):
        status = fields["status"].get("name") or ""
    status = status or bug.get("status") or bug.get("status_current") or "unknown"

    sev = ""
    if "priority" in fields and isinstance(fields["priority"], dict):
        sev = fields["priority"].get("name") or ""
    sev = sev or bug.get("severity") or "unknown"
    severity = norm_severity_generic(sev)

    resolution = ""
    if isinstance(fields.get("resolution"), dict):
        resolution = fields["resolution"].get("name") or ""
    else:
        resolution = safe_str(bug.get("resolution") or "")

    # texte
    summary = fields.get("summary") or bug.get("summary") or bug.get("title") or ""
    desc_blob, comments_len_chars = _extract_desc_from_jira(bug)
    desc_len = compute_desc_len(summary, desc_blob, comments_len_chars)

    # DERIVÉS — ***UNIQUEMENT TEXTE***
    keywords = extract_keywords_from_text(summary, desc_blob)
    security_flag = has_security_indicator(summary, desc_blob)
    leakage_flag, leakage_reasons = _leakage_from_status_resolution(status, resolution)

    # ids
    ticket_id = bug.get("id") or bug.get("ticket_id")
    ticket_id = safe_str(ticket_id) if ticket_id is not None else None
    raw_hash = sha256_obj(bug)

    return {
        "ticket_id": ticket_id,
        "key": key or (project + "-" + safe_str(ticket_id) if ticket_id else "BUGS-AUTO"),
        "created_at": created,
        "project": project or "BUGS",
        "component": component or "General",
        "severity": severity,
        "status_current": safe_str(status),
        "title_len": len(safe_str(summary)),
        "summary_len": len(safe_str(summary)),
        "text_len": len(safe_str(desc_blob)),
        "desc_len": int(desc_len),
        "summary_text": safe_str(summary),
        "desc_blob": safe_str(desc_blob),
        "keywords": keywords,               # OK (texte only)
        "security_flag": bool(security_flag),
        "resolution": safe_str(resolution), # QC only
        "leakage_flag": bool(leakage_flag), # QC only
        "leakage_reasons": leakage_reasons, # QC only
        "comments_len": int(comments_len_chars),
        "recent_incidents_1h": 0,
        "source_system": "jira",
        "raw_sha256": raw_hash
    }

def bugzilla_to_card(bug):
    bid  = bug.get("id") or bug.get("bug_id") or bug.get("ticket_id")
    project = safe_str(bug.get("product") or bug.get("project") or "BUGS")

    # component
    comp = bug.get("component")
    if isinstance(comp, list) and comp:
        comp = comp[0]
    comp = safe_str(comp or "General")

    # created
    created = bug.get("creation_time") or bug.get("created") or bug.get("created_at") or now_iso()
    created = safe_str(created)

    # texte
    summary = bug.get("summary") or bug.get("short_desc") or bug.get("title") or ""
    desc_blob, comments_len_chars = _extract_desc_from_bugzilla(bug)
    desc_len = compute_desc_len(summary, desc_blob, comments_len_chars)

    # severity / status / resolution
    severity   = norm_severity_bugzilla(bug.get("severity"))
    status     = safe_str(bug.get("status") or bug.get("status_current") or "unknown")
    resolution = safe_str(bug.get("resolution") or "")

    # DERIVÉS — ***UNIQUEMENT TEXTE***
    keywords = extract_keywords_from_text(summary, desc_blob)
    security_flag = has_security_indicator(summary, desc_blob)
    leakage_flag, leakage_reasons = _leakage_from_status_resolution(status, resolution)

    # ids
    key = safe_str(bug.get("key") or (project + "-" + safe_str(bid) if bid is not None else "BUG-AUTO"))
    ticket_id = safe_str(bid) if bid is not None else None
    raw_hash = sha256_obj(bug)

    return {
        "ticket_id": ticket_id,
        "key": key,
        "created_at": created,
        "project": project,
        "component": comp,
        "severity": severity,
        "status_current": status,           # QC
        "title_len": len(safe_str(summary)),
        "summary_len": len(safe_str(summary)),
        "text_len": len(safe_str(desc_blob)),
        "desc_len": int(desc_len),
        "summary_text": safe_str(summary),
        "desc_blob": safe_str(desc_blob),
        "keywords": keywords,               # OK (texte only)
        "security_flag": bool(security_flag),
        "resolution": resolution,           # QC
        "leakage_flag": bool(leakage_flag), # QC
        "leakage_reasons": leakage_reasons, # QC
        "comments_len": int(comments_len_chars),
        "recent_incidents_1h": 0,
        "source_system": "bugzilla",
        "raw_sha256": raw_hash
    }

def build_card_from_bug(bug):
    if is_jira(bug):      return jira_to_card(bug)
    if is_bugzilla(bug):  return bugzilla_to_card(bug)

    # Fallback générique (texte only pour keywords)
    project = safe_str(bug.get("project") or bug.get("product") or "BUGS")
    comp = bug.get("component") or bug.get("components") or "General"
    if isinstance(comp, list) and comp:
        comp = comp[0]
    comp = safe_str(comp)
    created = safe_str(bug.get("created_at") or bug.get("created") or now_iso())
    summary = bug.get("summary") or bug.get("title") or ""
    desc = bug.get("description") or bug.get("text") or ""
    desc_len = compute_desc_len(summary, desc, 0)
    severity = norm_severity_generic(bug.get("severity"))
    status = safe_str(bug.get("status") or bug.get("status_current") or "unknown")
    resolution = safe_str(bug.get("resolution") or "")

    kwords = extract_keywords_from_text(summary, desc)   # <- texte only
    sec_flag = has_security_indicator(summary, desc)
    leakage_flag, leakage_reasons = _leakage_from_status_resolution(status, resolution)

    bid = bug.get("id") or bug.get("bug_id") or bug.get("ticket_id")
    key = safe_str(bug.get("key") or (project + "-" + safe_str(bid) if bid is not None else "BUG-AUTO"))
    ticket_id = safe_str(bid) if bid is not None else None
    raw_hash = sha256_obj(bug)

    return {
        "ticket_id": ticket_id,
        "key": key,
        "created_at": created,
        "project": project,
        "component": comp,
        "severity": severity,
        "status_current": status,           # QC
        "title_len": len(safe_str(summary)),
        "summary_len": len(safe_str(summary)),
        "text_len": len(safe_str(desc)),
        "desc_len": int(desc_len),
        "summary_text": safe_str(summary),
        "desc_blob": safe_str(desc),
        "keywords": kwords,                 # OK (texte only)
        "security_flag": bool(sec_flag),
        "resolution": resolution,           # QC
        "leakage_flag": bool(leakage_flag), # QC
        "leakage_reasons": leakage_reasons, # QC
        "comments_len": 0,
        "recent_incidents_1h": 0,
        "source_system": "generic",
        "raw_sha256": raw_hash
    }

# -------- QC: missingness & duplicates --------
def write_missingness(out_dir, records):
    fields = set()
    for r in records: fields.update(r.keys())
    fields = sorted(fields)
    n = len(records)
    rows = []
    for f in fields:
        missing = sum(1 for r in records if (f not in r or r[f] in (None, "", [], {})))
        rows.append({"field": f, "missing": missing, "missing_rate": round(missing/n, 6) if n else 0.0})
    path = os.path.join(out_dir, "missingness.csv")
    with open(path, "w", newline="", encoding="utf-8") as fw:
        w = csv.DictWriter(fw, fieldnames=["field","missing","missing_rate"])
        w.writeheader(); w.writerows(rows)
    return path

def write_duplicates(out_dir, records):
    by_hash = defaultdict(list)
    for r in records:
        by_hash[r.get("raw_sha256","")].append(r)
    dupe_rows = []
    for h, arr in by_hash.items():
        if not h or len(arr) < 2: continue
        for r in arr:
            dupe_rows.append({
                "raw_sha256": h,
                "key": r.get("key",""),
                "ticket_id": r.get("ticket_id",""),
                "created_at": r.get("created_at",""),
                "summary_text": r.get("summary_text","")[:140]
            })
    path = os.path.join(out_dir, "duplicates.csv")
    with open(path, "w", newline="", encoding="utf-8") as fw:
        w = csv.DictWriter(fw, fieldnames=["raw_sha256","key","ticket_id","created_at","summary_text"])
        w.writeheader()
        w.writerows(dupe_rows)
    return path, len(dupe_rows)

# -------- Figures (matplotlib, sans style/couleur forcés) --------
def _plt():
    import matplotlib
    matplotlib.use("Agg")
    import matplotlib.pyplot as plt
    return plt

def fig_summary_len_hist(out_dir, records):
    plt = _plt()
    vals = [r.get("summary_len",0) for r in records]
    vals = [v for v in vals if isinstance(v, int)]
    plt.figure(figsize=(10,4.5))
    plt.hist(vals, bins=30)
    plt.xlabel("summary_len"); plt.ylabel("count"); plt.title("Summary length histogram")
    p = os.path.join(out_dir, "fig_summary_len_hist.png"); plt.savefig(p, bbox_inches="tight"); plt.close(); return p

def fig_status_dist(out_dir, records):
    plt = _plt()
    c = Counter(safe_str(r.get("status_current","unknown")).upper() for r in records)
    labs, vals = zip(*sorted(c.items(), key=lambda x: (-x[1], x[0]))) if c else ([],[])
    plt.figure(figsize=(10,4.5)); plt.bar(labs, vals); plt.xticks(rotation=45, ha="right")
    plt.ylabel("count"); plt.title("Status distribution")
    p = os.path.join(out_dir, "fig_status_dist.png"); plt.savefig(p, bbox_inches="tight"); plt.close(); return p

def fig_components_top10(out_dir, records):
    plt = _plt()
    c = Counter((safe_str(r.get("component","")).strip() or "General") for r in records)
    items = sorted(c.items(), key=lambda x: x[1], reverse=True)[:10]
    labs = [k for k,_ in items]; vals = [v for _,v in items]
    plt.figure(figsize=(12,4.5)); plt.bar(labs, vals); plt.xticks(rotation=45, ha="right")
    plt.ylabel("count"); plt.title("Top-10 components")
    p = os.path.join(out_dir, "fig_components_top10.png"); plt.savefig(p, bbox_inches="tight"); plt.close(); return p

def fig_created_months(out_dir, records):
    plt = _plt()
    def month_key(ts):
        s = safe_str(ts);
        return s[:7] if len(s)>=7 else "unknown"
    c = Counter(month_key(r.get("created_at","")) for r in records)
    items = sorted((k,v) for k,v in c.items() if k!="unknown")
    labs = [k for k,_ in items]; vals = [v for _,v in items]
    plt.figure(figsize=(12,4.5)); plt.plot(range(len(vals)), vals, marker="o")
    plt.xticks(range(len(labs)), labs, rotation=45, ha="right")
    plt.ylabel("count"); plt.title("Created per month")
    p = os.path.join(out_dir, "fig_created_months.png"); plt.savefig(p, bbox_inches="tight"); plt.close(); return p

def fig_missingness(out_dir, missingness_csv):
    plt = _plt()
    rows = []
    with open(missingness_csv, newline="", encoding="utf-8") as f:
        rdr = csv.DictReader(f)
        for r in rdr:
            rows.append((r["field"], float(r["missing_rate"])))
    rows = sorted(rows, key=lambda x: x[1], reverse=True)[:20]
    labs = [a for a,_ in rows]; vals = [b for _,b in rows]
    plt.figure(figsize=(12,5)); plt.bar(labs, vals); plt.xticks(rotation=60, ha="right")
    plt.ylabel("missing_rate"); plt.title("Top missingness")
    p = os.path.join(out_dir, "fig_missingness.png"); plt.savefig(p, bbox_inches="tight"); plt.close(); return p

def fig_leakage_rules(out_dir, records):
    plt = _plt()
    cc = Counter()
    for r in records:
        if r.get("leakage_flag"):
            for rea in r.get("leakage_reasons",[]) or []: cc[rea]+=1
    items = sorted(cc.items(), key=lambda x: x[1], reverse=True)
    labs = [k for k,_ in items]; vals = [v for _,v in items]
    plt.figure(figsize=(12,4.5))
    if labs:
        plt.bar(labs, vals); plt.xticks(rotation=45, ha="right")
    plt.ylabel("count"); plt.title("Leakage reasons")
    p = os.path.join(out_dir, "fig_leakage_rules.png"); plt.savefig(p, bbox_inches="tight"); plt.close(); return p

# -------- Allowlist --------
ALLOWLIST_FIELDS = [
    # Texte court + dérivés non post-hoc
    "summary_text","summary_len","title_len","desc_len","text_len",
    # Métadonnées “early”
    "project","component","severity","security_flag","keywords",
    # Temps brut (sera binned en aval si besoin)
    "created_at",
    # EXCLUS explicitement en aval: status_current, resolution, leakage_flag, leakage_reasons
]

def write_allowlist(out_dir):
    path = os.path.join(out_dir, "feature_allowlist.txt")
    with open(path, "w", encoding="utf-8") as f:
        f.write("# Features autorisées (anti-leakage)\n")
        for k in ALLOWLIST_FIELDS:
            f.write(k+"\n")
        f.write("\n# EXCLUS (post-hoc): status_current, resolution, leakage_flag, leakage_reasons\n")
    return path

# -------- DataCard & stats --------
def write_dataset_stats(out_dir, records, input_sha, figs, missing_csv, dup_count):
    stats = {
        "generated_at": now_iso(),
        "data_source_name": DATA_SOURCE_NAME,
        "data_source_url": DATA_SOURCE_URL,
        "data_access_date": DATA_ACCESS_DATE,
        "data_license": DATA_LICENSE,
        "data_license_url": DATA_LICENSE_URL,
        "input_path_sha256": input_sha,
        "cards": len(records),
        "source_system_counts": dict(Counter(r.get("source_system","") for r in records)),
        "desc_blob_empty_rate": round(sum(1 for r in records if not r.get("desc_blob"))/len(records), 6) if records else 0.0,
        "leakage_rate": round(sum(1 for r in records if r.get("leakage_flag"))/len(records), 6) if records else 0.0,
        "figures": [os.path.basename(p) for p in figs if p],
        "missingness_csv": os.path.basename(missing_csv),
        "duplicates_count_rows": dup_count
    }
    p = os.path.join(out_dir, "dataset_stats.json")
    with open(p, "w", encoding="utf-8") as fw:
        json.dump(stats, fw, ensure_ascii=False, indent=2)
    return p, stats

def write_datacard(out_dir, stats):
    md = f"""# DataCard — BugAda Cards (Étape 1, PROPRE)

- **Généré** : {stats.get('generated_at')}
- **Source** : {stats.get('data_source_name')} — {stats.get('data_source_url')}
- **Date d'accès** : {stats.get('data_access_date')}
- **SHA256 entrée** : {stats.get('input_path_sha256')}
- **Nombre de cartes** : {stats.get('cards')}
- **Leakage rate (QC)** : {stats.get('leakage_rate')}
- **% desc_blob vides** : {stats.get('desc_blob_empty_rate')}
- **Systèmes** : {stats.get('source_system_counts')}
- **Licence** : {stats.get('data_license')}  ({stats.get('data_license_url')})

## Politique anti-leakage
- Toute feature post-hoc (`status_current`, `resolution`) est **exclue** des features.
- Publication d’une **liste blanche** : `feature_allowlist.txt`.

## QC
- `missingness.csv` (taux de champs manquants ; figure jointe).
- `duplicates.csv` (doublons exacts par `raw_sha256`) — politique: *keep-first*.

## Figures pour papier
- `fig_summary_len_hist.png`
- `fig_created_months.png`
- `fig_components_top10.png`
- `fig_missingness.png`
- `fig_leakage_rules.png`

## Note
Si `desc_blob` est vide dans la source, les modèles en aval se contenteront du **titre** + métadonnées *non post-hoc*. C’est attendu et documenté.
"""
    p = os.path.join(out_dir, "DATACARD.md")
    with open(p, "w", encoding="utf-8") as f:
        f.write(md)
    return p

# -------- Main conversion --------
def convert_to_cards(input_path, out_dir, limit=None):
    ensure_dir(out_dir)
    bugs = read_json_any(input_path)
    if not isinstance(bugs, list) or len(bugs) == 0:
        print("[ERR] Aucune donnée lisible."); sys.exit(1)
    if len(bugs) == 1:
        print("[WARN] Seulement 1 élément détecté. Vérifie le format d'entrée.")

    out_jsonl = os.path.join(out_dir, "episodes_raw.jsonl")
    out_sample = os.path.join(out_dir, "episodes_raw.sample.json")
    n = 0; sample = []; cards = []; c_desc_blob_empty = 0

    with open(out_jsonl, "w", encoding="utf-8") as fw:
        for bug in bugs:
            try:
                card = build_card_from_bug(bug)
            except Exception:
                continue
            fw.write(json.dumps(card, ensure_ascii=False) + "\n")
            cards.append(card)
            if not card.get("desc_blob"): c_desc_blob_empty += 1
            if len(sample) < 5: sample.append(card)
            n += 1
            if limit and n >= limit: break

    with open(out_sample, "w", encoding="utf-8") as fs:
        json.dump(sample, fs, ensure_ascii=False, indent=2)

    # Aperçu console (3 cartes)
    print("Aperçu de 3 cartes:")
    for r in cards[:3]:
        print(json.dumps(r, ensure_ascii=False))

    print(f"[STATS] cartes: {n}")
    if n: print(f"[STATS] %desc_blob vide (global): {c_desc_blob_empty/n:.2%}")

    # QC
    miss_csv = write_missingness(out_dir, cards)
    dup_csv, dup_rows = write_duplicates(out_dir, cards)

    # Figures (papier)
    figs = []
    figs.append(fig_summary_len_hist(out_dir, cards))
    figs.append(fig_status_dist(out_dir, cards))
    figs.append(fig_components_top10(out_dir, cards))
    figs.append(fig_created_months(out_dir, cards))
    figs.append(fig_missingness(out_dir, miss_csv))
    figs.append(fig_leakage_rules(out_dir, cards))
    print(f"[FIG] Graphiques écrits dans: {out_dir}")

    # Paper assets
    paper_dir = os.path.join(out_dir, "paper_assets"); ensure_dir(paper_dir)
    for p in figs:
        if p: shutil.copy2(p, os.path.join(paper_dir, os.path.basename(p)))
    print(f"[PAPER] assets copiés -> {paper_dir}")

    # Allowlist
    allow_path = write_allowlist(out_dir)

    # Stats + datacard
    input_sha = sha256_file(input_path)
    stats_path, stats = write_dataset_stats(out_dir, cards, input_sha, figs, miss_csv, dup_rows)
    dc_path = write_datacard(out_dir, stats)

    print(f"- JSONL    : {out_jsonl}")
    print(f"- Échantill: {out_sample}")
    print(f"[QC] missingness.csv | duplicates.csv écrits.")
    print(f"[STATS] dataset_stats.json & DATACARD.md écrits.")
    print(f"[ALLOW] {allow_path}")

if __name__ == "__main__":
    convert_to_cards(INPUT_PATH, OUT_DIR, LIMIT)